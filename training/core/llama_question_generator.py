"""
Enhanced Llama question generator with unlimited high-quality generation using Ollama 3.2
"""

import asyncio
import logging
import json
import re
from typing import List, Dict
from datetime import datetime
import aiohttp

from utils.quality_controller import QualityController

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LlamaQuestionGenerator:
    def __init__(self):
        self.quality_controller = QualityController()
        self.ollama_url = "http://localhost:11434"
        self.model_name = "llama3.2"
        
    async def generate_questions_and_answers(self, chunk_data: Dict) -> List[Dict]:
        """
        Generate unlimited high-quality Q&A pairs using Ollama 3.2 with complete freedom
        """
        content = chunk_data["content"]
        resource_id = chunk_data["resource_id"]
        chunk_id = chunk_data["chunk_id"]
        
        logger.info(f"🧠 Generating unlimited Q&A with Ollama 3.2 for {resource_id} chunk {chunk_id}")
        
        try:
            # Check if Ollama is available
            if not await self._check_ollama_available():
                logger.warning("Ollama not available, falling back to enhanced templates")
                return await self._fallback_generation(content, resource_id, chunk_id)
            
            # Generate comprehensive questions using Ollama with complete freedom
            questions = await self._generate_unlimited_questions_with_ollama(content)
            
            if not questions:
                logger.warning("No questions generated by Ollama, using fallback")
                return await self._fallback_generation(content, resource_id, chunk_id)
            
            logger.info(f"🎯 Ollama generated {len(questions)} questions, now generating answers...")
            
            # Generate answers for each question
            qa_pairs = []
            for i, question in enumerate(questions):
                
                # Generate answer using Ollama
                answer = await self._generate_answer_with_ollama(question, content)
                
                # Assess quality
                question_quality = self.quality_controller.assess_question_quality(question, content)
                answer_quality = await self._assess_answer_quality(answer, content)
                overall_quality = (question_quality * 0.6 + answer_quality * 0.4)
                
                # Only include high-quality pairs (higher threshold for unlimited generation)
                if overall_quality >= 0.75 and len(question) >= 15 and len(answer) >= 20:
                    qa_pair = {
                        "instruction": question.strip(),
                        "input": f"Context: {content[:300]}...",
                        "output": answer.strip(),
                        
                        # Metadata
                        "resource_id": resource_id,
                        "chunk_id": chunk_id,
                        "qa_pair_id": f"{resource_id}_chunk{chunk_id}_qa{i}",
                        "generation_timestamp": datetime.now().isoformat(),
                        
                        # Quality scores
                        "question_quality": question_quality,
                        "answer_quality": answer_quality,
                        "overall_quality": overall_quality,
                        "confidence": min(1.0, overall_quality + 0.1),
                        
                        # Generation metadata
                        "generation_method": "ollama_3.2_unlimited",
                        "model_used": self.model_name,
                        "complexity": self._assess_complexity(question),
                        "question_type": self._classify_question_type(question)
                    }
                    qa_pairs.append(qa_pair)
            
            logger.info(f"✅ Generated {len(qa_pairs)} high-quality Q&A pairs from {len(questions)} total questions")
            return qa_pairs
            
        except Exception as e:
            logger.error(f"❌ Unlimited Q&A generation failed: {e}")
            return await self._fallback_generation(content, resource_id, chunk_id)
    
    async def _check_ollama_available(self) -> bool:
        """Check if Ollama is running and model is available"""
        try:
            async with aiohttp.ClientSession() as session:
                # Check if Ollama is running
                async with session.get(f"{self.ollama_url}/api/tags", timeout=5) as resp:
                    if resp.status != 200:
                        return False
                    
                    tags_data = await resp.json()
                    models = [model['name'] for model in tags_data.get('models', [])]
                    
                    # Check if our model is available
                    return any(self.model_name in model for model in models)
                    
        except Exception as e:
            logger.error(f"Ollama availability check failed: {e}")
            return False
    
    async def _generate_unlimited_questions_with_ollama(self, content: str) -> List[str]:
        """Generate unlimited questions with complete freedom using Ollama 3.2"""
        
        comprehensive_prompt = f"""You are an expert question generator with complete freedom to create as many high-quality, diverse questions as you deem appropriate based on the richness of the provided content.

CONTENT TO ANALYZE:
{content}

YOUR MISSION: Generate as many excellent questions as this content can support - whether that's 10, 20, 30, 50, or more. You decide based on the content's depth and potential for meaningful questions.

COMPREHENSIVE GENERATION GUIDELINES:

1. EXTRACT MAXIMUM VALUE:
   • Mine every detail, fact, number, name, process, and concept
   • Create questions for both explicit information and implied relationships
   • Cover main topics, subtopics, supporting details, and connections
   • Address facts, processes, implications, comparisons, and applications

2. QUESTION DIVERSITY (use ALL these types extensively):
   • FACTUAL: What, which, who, where questions about specifics
   • QUANTITATIVE: Questions about numbers, percentages, amounts, targets
   • TEMPORAL: When, timing, sequence, phase, duration questions  
   • ANALYTICAL: How, why questions about processes and reasoning
   • COMPARATIVE: Questions comparing elements, approaches, phases
   • INFERENTIAL: Questions requiring logical deduction
   • APPLICATION: How concepts apply to scenarios or outcomes
   • CAUSAL: Questions about cause-and-effect relationships
   • DEFINITIONAL: Questions about terminology and concepts
   • EVALUATIVE: Questions about effectiveness, benefits, limitations

3. COMPLEXITY SPECTRUM (generate questions at ALL levels):
   • SIMPLE: Direct fact retrieval from text
   • INTERMEDIATE: Understanding relationships between concepts
   • ADVANCED: Analysis, synthesis, and inference from content
   • EXPERT: Multi-layered questions requiring deep comprehension

4. COMPREHENSIVE COVERAGE:
   • Address EVERY significant topic mentioned
   • Create multiple questions per major concept
   • Don't miss any important details or numbers
   • Include questions about processes, outcomes, and implications
   • Generate questions about context, background, and significance

5. QUALITY STANDARDS:
   • Each question must be complete, grammatically perfect, and specific
   • Use exact names, numbers, and terminology from the content
   • Ensure every question is answerable from the provided content
   • Avoid vague or generic questions - be precise and detailed
   • Vary question length and complexity naturally

6. UNLIMITED FREEDOM:
   • Generate as many questions as the content supports
   • Don't stop at arbitrary limits - exhaust the content's potential
   • If the content is rich, generate 30+ questions
   • If content is detailed, create 40+ questions  
   • Continue until you've covered everything meaningful

FORMAT: Generate your questions numbered sequentially (Q1:, Q2:, Q3:, etc.) until you have exhaustively covered all aspects of the content.

START GENERATING - CREATE AS MANY HIGH-QUALITY QUESTIONS AS THIS CONTENT DESERVES:

Q1:"""
        
        try:
            async with aiohttp.ClientSession() as session:
                payload = {
                    "model": self.model_name,
                    "prompt": comprehensive_prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.8,
                        "top_p": 0.9,
                        "top_k": 50,
                        "repeat_penalty": 1.1,
                        "max_tokens": 2000,  # Large token limit for unlimited generation
                        "num_predict": 2000  # Ollama-specific parameter for max output
                    }
                }
                
                async with session.post(f"{self.ollama_url}/api/generate", 
                                      json=payload, timeout=180) as resp:  # Extended timeout
                    if resp.status != 200:
                        raise Exception(f"Ollama API error: {resp.status}")
                    
                    result = await resp.json()
                    generated_text = result.get('response', '')
                    
                    # Parse all generated questions
                    questions = self._parse_unlimited_questions(generated_text)
                    
                    # If we got a substantial number, try one more generation for completeness
                    if len(questions) >= 15:
                        additional_questions = await self._generate_supplementary_questions(content, questions)
                        questions.extend(additional_questions)
                    
                    # Remove duplicates while preserving quality
                    unique_questions = self._deduplicate_questions(questions)
                    
                    logger.info(f"🎯 Generated {len(unique_questions)} unique questions from unlimited generation")
                    return unique_questions
                    
        except Exception as e:
            logger.error(f"Unlimited question generation failed: {e}")
            return []

    async def _generate_supplementary_questions(self, content: str, existing_questions: List[str]) -> List[str]:
        """Generate additional questions to ensure complete coverage"""
        
        existing_topics = self._extract_topics_from_questions(existing_questions)
        
        supplementary_prompt = f"""You have already generated {len(existing_questions)} questions covering these topics: {', '.join(existing_topics[:10])}

Now, analyze the content again and generate additional high-quality questions that:
1. Cover any aspects not addressed by existing questions
2. Explore deeper implications and relationships
3. Focus on connections between different concepts
4. Address any numerical data or specific details that might have been missed
5. Create advanced analytical questions requiring synthesis

Content: {content}

Generate as many additional unique, high-quality questions as you can find gaps for:

Q1:"""
        
        try:
            async with aiohttp.ClientSession() as session:
                payload = {
                    "model": self.model_name,
                    "prompt": supplementary_prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.9,
                        "top_p": 0.95,
                        "max_tokens": 1000,
                        "num_predict": 1000
                    }
                }
                
                async with session.post(f"{self.ollama_url}/api/generate", 
                                      json=payload, timeout=120) as resp:
                    if resp.status == 200:
                        result = await resp.json()
                        generated_text = result.get('response', '')
                        return self._parse_unlimited_questions(generated_text)
                
        except Exception as e:
            logger.error(f"Supplementary question generation failed: {e}")
        
        return []

    def _extract_topics_from_questions(self, questions: List[str]) -> List[str]:
        """Extract main topics from existing questions"""
        topics = []
        for question in questions:
            # Extract key terms (capitalize words, numbers, specific terms)
            words = re.findall(r'\b[A-Z][a-zA-Z]*\b|\b\d+%?\b|\b[a-z]{4,}\b', question)
            topics.extend(words[:3])  # Take first 3 meaningful terms per question
        
        # Return unique topics
        return list(set(topics))[:20]

    def _parse_unlimited_questions(self, response: str) -> List[str]:
        """Enhanced question parsing for unlimited generation"""
        questions = []
        
        lines = response.split('\n')
        
        for line in lines:
            line = line.strip()
            
            # Multiple parsing strategies for different formats
            patterns = [
                r'^Q\d+:\s*(.+?)(?:\?|$)',  # Q1: question?
                r'^\d+[\.\)]\s*(.+?)(?:\?|$)',  # 1. question? or 1) question?
                r'^[-•*]\s*(.+?)(?:\?|$)',  # - question? or • question?
                r'^(.+\?)$'  # Direct questions ending with ?
            ]
            
            for pattern in patterns:
                match = re.match(pattern, line)
                if match:
                    question = match.group(1).strip()
                    
                    # Ensure question ends with ?
                    if not question.endswith('?'):
                        question += '?'
                    
                    # Quality filters
                    if (len(question) >= 15 and 
                        len(question) <= 200 and 
                        question.count('?') == 1 and
                        not question.lower().startswith(('generate', 'create', 'make'))):
                        
                        questions.append(question)
                    break
        
        return questions

    def _deduplicate_questions(self, questions: List[str]) -> List[str]:
        """Remove duplicates while preserving quality and order"""
        seen = set()
        unique_questions = []
        
        for question in questions:
            # Normalize for comparison (lowercase, remove extra spaces)
            normalized = re.sub(r'\s+', ' ', question.lower().strip())
            
            # Check for substantial similarity (not just exact matches)
            is_duplicate = False
            for seen_q in seen:
                # Calculate simple similarity
                common_words = set(normalized.split()) & set(seen_q.split())
                if len(common_words) >= min(len(normalized.split()), len(seen_q.split())) * 0.7:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                seen.add(normalized)
                unique_questions.append(question)
        
        return unique_questions

    async def _generate_answer_with_ollama(self, question: str, context: str) -> str:
        """Generate comprehensive answer using Ollama 3.2"""
        
        answer_prompt = f"""Based on the provided context, provide a comprehensive, accurate, and detailed answer to the following question. Use only information from the context.

Context:
{context}

Question: {question}

Instructions for your answer:
- Be thorough and detailed while staying accurate to the context
- Include specific numbers, names, and details mentioned in the context
- If the context provides partial information, acknowledge what is and isn't specified
- Organize your answer clearly and logically
- Aim for 2-4 complete sentences for comprehensive coverage

Answer:"""
        
        try:
            async with aiohttp.ClientSession() as session:
                payload = {
                    "model": self.model_name,
                    "prompt": answer_prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.3,  # Lower temperature for factual answers
                        "top_p": 0.8,
                        "max_tokens": 500,
                        "num_predict": 500
                    }
                }
                
                async with session.post(f"{self.ollama_url}/api/generate", 
                                      json=payload, timeout=90) as resp:
                    if resp.status != 200:
                        raise Exception(f"Ollama API error: {resp.status}")
                    
                    result = await resp.json()
                    answer = result.get('response', '').strip()
                    
                    # Clean up the answer
                    answer = self._clean_answer(answer)
                    return answer
                    
        except Exception as e:
            logger.error(f"Answer generation with Ollama failed: {e}")
            return "Unable to generate answer due to processing error."
    
    def _clean_answer(self, answer: str) -> str:
        """Clean and format generated answer"""
        # Remove common prefixes
        prefixes = ['Answer:', 'A:', 'Response:', 'Based on the context,', 'According to the context,']
        
        for prefix in prefixes:
            if answer.startswith(prefix):
                answer = answer[len(prefix):].strip()
        
        # Ensure proper sentence structure
        if answer and not answer.endswith(('.', '!', '?')):
            answer += '.'
        
        # Remove excessive length while preserving completeness
        if len(answer) > 1000:
            sentences = answer.split('.')
            answer = '. '.join(sentences[:4]) + '.'
        
        return answer
    
    async def _assess_answer_quality(self, answer: str, context: str) -> float:
        """Assess the quality of generated answer"""
        score = 0.5
        
        # Length appropriateness
        if 40 <= len(answer) <= 800:
            score += 0.25
        
        # Context relevance check
        answer_words = set(answer.lower().split())
        context_words = set(context.lower().split())
        overlap = len(answer_words & context_words)
        
        if overlap >= 8:
            score += 0.25
        elif overlap >= 5:
            score += 0.15
        
        return min(1.0, score)
    
    async def _fallback_generation(self, content: str, resource_id: str, chunk_id: int) -> List[Dict]:
        """Enhanced fallback generation if Ollama fails"""
        logger.info("Using enhanced template fallback generation")
        
        # Extract comprehensive key terms from content
        key_terms = self._extract_comprehensive_terms(content)
        
        # Generate multiple question types based on content analysis
        questions = []
        
        # Factual questions
        if key_terms.get('numbers'):
            for num in key_terms['numbers'][:3]:
                questions.append(f"What is the significance of {num} mentioned in the content?")
        
        if key_terms.get('names'):
            for name in key_terms['names'][:3]:
                questions.append(f"What is {name} and what role does it play?")
        
        # Process questions
        if any(word in content.lower() for word in ['phase', 'step', 'stage', 'process']):
            questions.extend([
                "What are the main phases or stages described?",
                "What is the current status of the process mentioned?",
                "What are the next steps in the process?"
            ])
        
        # Objective/goal questions
        if any(word in content.lower() for word in ['objective', 'goal', 'aim', 'target']):
            questions.extend([
                "What are the primary objectives mentioned?",
                "What goals are being pursued?",
                "What outcomes are expected?"
            ])
        
        # Generate QA pairs
        qa_pairs = []
        for i, question in enumerate(questions[:15]):  # Limit fallback to 15
            answer = self._extract_contextual_answer(question, content)
            
            qa_pair = {
                "instruction": question,
                "input": f"Context: {content[:300]}...",
                "output": answer,
                "resource_id": resource_id,
                "chunk_id": chunk_id,
                "qa_pair_id": f"{resource_id}_chunk{chunk_id}_qa{i}",
                "generation_timestamp": datetime.now().isoformat(),
                "question_quality": 0.7,
                "answer_quality": 0.65,
                "overall_quality": 0.67,
                "confidence": 0.7,
                "generation_method": "enhanced_template_fallback",
                "complexity": "medium",
                "question_type": "contextual"
            }
            qa_pairs.append(qa_pair)
        
        return qa_pairs
    
    def _extract_comprehensive_terms(self, content: str) -> Dict[str, List[str]]:
        """Extract comprehensive key terms from content"""
        terms = {
            'names': re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', content)[:10],
            'numbers': re.findall(r'\b\d+%?\b', content)[:5],
            'processes': re.findall(r'\b(?:process|method|approach|strategy|technique)\w*\b', content, re.I)[:5]
        }
        
        return terms
    
    def _extract_contextual_answer(self, question: str, content: str) -> str:
        """Extract contextual answer from content"""
        sentences = content.split('.')
        relevant_sentences = []
        
        # Extract question keywords
        question_words = set(re.findall(r'\b[a-zA-Z]{3,}\b', question.lower()))
        stop_words = {'what', 'how', 'when', 'where', 'why', 'is', 'are', 'the', 'and', 'or', 'but'}
        question_words -= stop_words
        
        # Find most relevant sentences
        for sentence in sentences:
            sentence_words = set(re.findall(r'\b[a-zA-Z]{3,}\b', sentence.lower()))
            overlap = len(question_words & sentence_words)
            
            if overlap >= 1 and len(sentence.strip()) > 20:
                relevant_sentences.append(sentence.strip())
        
        if relevant_sentences:
            # Return up to 3 most relevant sentences
            return '. '.join(relevant_sentences[:3]) + '.'
        
        # Fallback to first substantial sentences
        substantial_sentences = [s.strip() for s in sentences if len(s.strip()) > 30]
        if substantial_sentences:
            return '. '.join(substantial_sentences[:2]) + '.'
        
        return "The information is not clearly specified in the provided content."
    
    def _assess_complexity(self, question: str) -> str:
        """Assess question complexity"""
        word_count = len(question.split())
        if word_count <= 12:
            return "simple"
        elif word_count <= 20:
            return "medium"
        else:
            return "complex"
    
    def _classify_question_type(self, question: str) -> str:
        """Classify question type"""
        question_lower = question.lower()
        
        if question_lower.startswith(('what', 'which')):
            return "factual"
        elif question_lower.startswith(('how', 'why')):
            return "explanatory"
        elif question_lower.startswith(('when', 'where')):
            return "contextual"
        else:
            return "general"
